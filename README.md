# WIDS Assignments (Week 1 – Week 4)

## Project Title  
**Tokens to Thought — A Contextual Transformer**

---

## Project Overview

This repository contains all assignments and projects completed from Week 1 to Week 4 as part of the WIDS program. The main objective of this project is to understand the fundamentals of machine learning and deep learning and apply them through practical implementation.

The learning process starts from basic concepts and gradually progresses to advanced models such as Transformers for text generation.

---

## Key Features

- Understanding basic machine learning concepts  
- Learning neural network fundamentals  
- Implementing image classification using TensorFlow  
- Creating custom deep learning layers  
- Developing a Transformer model for text generation  

---

## Week-Wise Breakdown

---

### Week 1 — Introduction to Machine Learning

#### Description  
In Week 1, the focus was on learning the basic concepts of machine learning and data analysis.

#### Main Activities
- Introduction to Python for data science  
- Understanding datasets and features  
- Basic data preprocessing techniques  
- Learning how machine learning models work  
- Implementing simple programs for practice  

#### Learning Outcome  
This week helped in building a strong foundation in machine learning and data handling.

---

### Week 2 — Neural Networks and Deep Learning Basics

#### Description  
In Week 2, the focus was on understanding how neural networks work internally.

#### Main Activities
- Learning about neurons, layers, and activation functions  
- Understanding forward propagation and backpropagation  
- Implementing simple neural networks from scratch  
- Studying loss functions and optimizers  

#### Learning Outcome  
This week helped in understanding how neural networks learn from data and improve their performance.

---

### Week 3 — Image Classification Using TensorFlow

#### Description  
In Week 3, neural networks were implemented using the TensorFlow library for image classification tasks.

#### Main Activities
- MNIST handwritten digit classification using built-in layers  
- MNIST classification using custom layers  
- Creating custom Dense and Flatten layers  
- Training and evaluating models  
- Plotting accuracy graphs  
- Analyzing model performance  

#### Learning Outcome  
This week provided practical experience in building and training deep learning models using TensorFlow.

---

### Week 4 — Transformer for Text Generation

#### Description  
In Week 4, advanced natural language processing techniques were studied using Transformer models.

#### Main Activities
- Implementing a Transformer-based model in TensorFlow  
- Training the model on Shakespeare text dataset  
- Using positional embeddings and self-attention mechanisms  
- Generating new text in Shakespearean writing style  
- Optimizing training based on system limitations  

#### Learning Outcome  
This week helped in understanding modern NLP techniques and how Transformers are used for text generation.

---

## Overall Learning Outcomes

Through this project, I learned:

- Basic and advanced concepts of machine learning  
- Working of neural networks and deep learning models  
- Practical implementation using TensorFlow  
- Image classification using MNIST dataset  
- Text generation using Transformer models  
- Training, evaluating, and optimizing neural networks  

---

## Tools and Technologies Used

- Programming Language: Python  
- Deep Learning Framework: TensorFlow, Keras  
- Libraries: NumPy, Matplotlib  
- Platform: Google Colab / Jupyter Notebook  

---

## Conclusion

This project represents my learning journey from basic machine learning concepts to advanced deep learning models. By completing the assignments from Week 1 to Week 4, I gained hands-on experience in building, training, and evaluating neural networks and Transformer models. This project helped me strengthen my foundation in artificial intelligence, data science, and practical machine learning applications.


